{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3b662d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1228c62d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import pyvene as pv\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set a random seed so later our outputs dont keep changing\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9255cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyvene import IntervenableModel\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load gpt-2\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "model.eval() \n",
    "\n",
    "# wrap it in pyvene so we can analyze it later\n",
    "empty_config = {}\n",
    "pv_model = IntervenableModel(empty_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87cae0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts for subject verb agreement:\n",
      "  'The cat' → should prefer ' is'\n",
      "  'The cats' → should prefer ' are'\n",
      "  'The boy' → should prefer ' is'\n",
      "  'The boys' → should prefer ' are'\n",
      "  'The dog' → should prefer ' is'\n",
      "  'The dogs' → should prefer ' are'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "i want to investigate which neurons are responsible for subject verb agreement\n",
    "so just adding in some simple test cases for that here. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "agreement_sentences = [\n",
    "    (\"The cat\", \" is\"),\n",
    "    (\"The cats\", \" are\"),\n",
    "    (\"The boy\", \" is\"),\n",
    "    (\"The boys\", \" are\"),\n",
    "    (\"The dog\", \" is\"),\n",
    "    (\"The dogs\", \" are\")\n",
    "]\n",
    "\n",
    "print(\"Prompts for subject verb agreement:\")\n",
    "for prompt, correct in agreement_sentences:\n",
    "    print(f\"  {prompt!r} → should prefer {correct!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a17fb89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The cat': P(is) = 0.000081, P(are) = 0.000003\n",
      "'The cats': P(are) = 0.000011, P(is) = 0.000026\n",
      "'The boy': P(is) = 0.000070, P(are) = 0.000002\n",
      "'The boys': P(are) = 0.000004, P(is) = 0.000028\n",
      "'The dog': P(is) = 0.000078, P(are) = 0.000002\n",
      "'The dogs': P(are) = 0.000008, P(is) = 0.000024\n"
     ]
    }
   ],
   "source": [
    "# Get the baseline so we can compare to after we introduce noise\n",
    "baseline_probs = []\n",
    "\n",
    "for prompt, correct in agreement_sentences:\n",
    "    # tokenize the prompt + a space if needed\n",
    "    text = prompt + \" \"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # run forward pass (no intervention yet!)\n",
    "    with torch.no_grad():\n",
    "        outputs = pv_model.model(**inputs)\n",
    "\n",
    "    logits = outputs.logits[0]  # remove batch dim since it's always 1\n",
    "\n",
    "    # get ids for both the correct and incorrect next tokens\n",
    "    # correct is like \" is\" and incorrect would be the opposite\n",
    "    correct_id = tokenizer.encode(correct.strip(), add_special_tokens=False)[0]\n",
    "\n",
    "    # we pick one obvious incorrect opposite: if correct is \"is\", pick \"are\" and vice versa\n",
    "    wrong = \" are\" if correct.strip() == \"is\" else \" is\"\n",
    "    wrong_id = tokenizer.encode(wrong.strip(), add_special_tokens=False)[0]\n",
    "\n",
    "    # probability of correct vs wrong using softmax\n",
    "    probs = torch.softmax(logits[-1], dim=-1)\n",
    "    correct_prob = probs[correct_id].item()\n",
    "    wrong_prob = probs[wrong_id].item()\n",
    "\n",
    "    baseline_probs.append((prompt, correct, correct_prob, wrong_prob))\n",
    "\n",
    "    # i need to print to more decimals to see which option the model prefers, because it obviously\n",
    "    # spread its probability among many thousands of possible next tokens\n",
    "    print(f\"{prompt!r}: P({correct.strip()}) = {correct_prob:.6f}, \"\n",
    "      f\"P({wrong.strip()}) = {wrong_prob:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd2d5f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervention ready: zeroing neuron 42 in transformer.h.0.mlp.c_fc\n"
     ]
    }
   ],
   "source": [
    "# We can now zero out one neuron and see how this affects the model's behavior\n",
    "\n",
    "layer_name = \"transformer.h.0.mlp.c_fc\"  # first MLP in GPT-2\n",
    "neuron_index = 42  # arbitrary neuron to intervene on\n",
    "\n",
    "# This function will be applied to the activations at the chosen layer\n",
    "def zero_one_neuron(tensor, _):\n",
    "    # tensor is the activation output of the layer\n",
    "    tensor[..., neuron_index] = 0.0\n",
    "    return tensor\n",
    "\n",
    "# Define the intervention using PyVene's Intervention object\n",
    "intervention = pv.Intervention(\n",
    "    target=layer_name,\n",
    "    intervention_fn=zero_one_neuron,\n",
    "    rep_type=\"output\"  # intervene on the output of the module\n",
    ")\n",
    "\n",
    "print(f\"Intervention ready: zeroing neuron {neuron_index} in {layer_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f5fe3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
